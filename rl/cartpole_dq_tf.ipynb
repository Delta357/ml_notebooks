{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02257066, -0.04712394, -0.04959457, -0.01279513])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 possible actions.\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space.n\n",
    "print(f\"There are {action_space} possible actions.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                 size=batch_size,\n",
    "                                 replace=False)\n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 8, 5, 1, 3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem = Memory(10)\n",
    "[mem.add(i) for i in np.arange(10)]\n",
    "mem.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 600\n",
    "pretrain_length = 10\n",
    "memory = Memory(max_size=MEMORY_SIZE)\n",
    "\n",
    "state = env.reset() #env.decode(env.reset()))\n",
    "\n",
    "done = False\n",
    "step_limit = 600\n",
    "step = 0\n",
    "while step < step_limit:\n",
    "    \n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(random_action)\n",
    "    \n",
    "    \n",
    "    memory.add((state, random_action, new_state, reward, done, info))\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset() \n",
    "        \n",
    "    else:\n",
    "        state = new_state\n",
    "        \n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([-0.04995308,  0.34708712,  0.02379961, -0.45910456]),\n",
       "  1,\n",
       "  array([-0.04301134,  0.5418647 ,  0.01461752, -0.74419176]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {}),\n",
       " (array([ 0.01873945,  0.79360303,  0.08274351, -0.70245909]),\n",
       "  1,\n",
       "  array([ 0.03461151,  0.98748661,  0.06869433, -0.96799062]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {}),\n",
       " (array([ 0.00526792, -0.62564526, -0.01350803,  0.7952413 ]),\n",
       "  0,\n",
       "  array([-0.00724499, -0.82057924,  0.0023968 ,  1.08364436]),\n",
       "  1.0,\n",
       "  False,\n",
       "  {})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[-0.03683413, -0.0183279 ],\n",
       "       [-0.20276211, -0.2177669 ],\n",
       "       [ 0.04845861, -0.1822306 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the Q value of each action given the state\n",
    "inputs = layers.Input(shape=(4,))\n",
    "x = layers.Dense(50, activation=\"relu\")(inputs)\n",
    "x = layers.Dense(50, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(2, activation=\"linear\")(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model_target = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model.output_shape\n",
    "model(tf.random.uniform((3,4),-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(episode, model, state, min_epsilon, max_epsilon, decay_rate):\n",
    "\n",
    "    # random number for explore/exploit trade-off\n",
    "    epsilon = np.random.rand()\n",
    "\n",
    "    # current ee prob\n",
    "    explore_prob = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
    "\n",
    "    if epsilon < explore_prob:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        q_vals = model(tf.expand_dims(state, axis=0))\n",
    "        action = np.argmax(q_vals)\n",
    "\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "[select_action(1, model, tf.random.normal((4,)), 0.0, 0.0, 0.01) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.9613245 , 0.84012526, 0.8266217 ], dtype=float32)>,\n",
       " array([[ 2.70633112e-02,  4.08537777e-02,  7.50476390e-05,\n",
       "          2.91429823e-02],\n",
       "        [ 1.76725186e-02, -5.73411478e-01, -9.76108902e-03,\n",
       "          7.57096472e-01],\n",
       "        [-9.16366193e-03, -5.73347427e-01,  2.63146790e-02,\n",
       "          7.55702943e-01]]),\n",
       " [0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_minibatch(model, memory, gamma, batch_size):\n",
    "\n",
    "    # memory structure: (state, action, new_state, reward, done, info)\n",
    "\n",
    "    batch = memory.sample(batch_size)\n",
    "    states = np.array([each[0] for each in batch])\n",
    "    actions =[each[1] for each in batch]\n",
    "    next_states = np.array([each[2] for each in batch])\n",
    "    rewards = [each[3] for each in batch]\n",
    "    dones = tf.constant([each[4] for each in batch], dtype=tf.float32)\n",
    "\n",
    "    # get q values from target model\n",
    "    q_target = model(next_states)\n",
    "    \n",
    "    q_target = tf.reduce_max(q_target, axis=1)\n",
    "    # set done q_target = reward and discount the others\n",
    "    q_target = rewards + gamma * (1. - dones) * q_target\n",
    "\n",
    "    return q_target, states, actions\n",
    "\n",
    "get_minibatch(model, memory, 0.99, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 10000       # Total number of training episodes\n",
    "max_steps = 200               # Max steps per episode\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 0.01          # Learning rate\n",
    "gamma = 0.9                  # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 1.0               # Exploration rate\n",
    "max_epsilon = 1             # Exploration probability at start\n",
    "min_epsilon = .1            # Minimum exploration probability \n",
    "decay_rate = 0.003          # Exponential decay rate for exploration prob\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def explore(min_epsilon, max_epsilon, decay_rate, episode):\n",
    "    return min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
    "    \n",
    "\n",
    "plt.plot(np.arange(1000), [explore(min_epsilon, max_epsilon, decay_rate, e) for e in np.arange(1000)])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\tLoss: 0.862\tEpisode reward: 36.0\n",
      "Episode: 100\tLoss: 3.630\tEpisode reward: 14.0\n",
      "Episode: 200\tLoss: 1.495\tEpisode reward: 26.0\n",
      "Episode: 300\tLoss: 0.930\tEpisode reward: 78.0\n",
      "Episode: 400\tLoss: 0.515\tEpisode reward: 28.0\n",
      "Episode: 500\tLoss: 0.091\tEpisode reward: 126.0\n",
      "Episode: 600\tLoss: 0.452\tEpisode reward: 182.0\n",
      "Episode: 700\tLoss: 0.016\tEpisode reward: 293.0\n",
      "Episode: 800\tLoss: 0.014\tEpisode reward: 357.0\n",
      "Episode: 900\tLoss: 0.470\tEpisode reward: 192.0\n",
      "Episode: 1000\tLoss: 0.020\tEpisode reward: 208.0\n",
      "Episode: 1100\tLoss: 0.686\tEpisode reward: 282.0\n",
      "Episode: 1200\tLoss: 0.006\tEpisode reward: 474.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d706e73a8aaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mnew_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-fda180078e2b>\u001b[0m in \u001b[0;36mselect_action\u001b[1;34m(episode, model, state, min_epsilon, max_epsilon, decay_rate)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mq_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mexpand_dims_v2\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m    434\u001b[0m     \u001b[0mInvalidArgumentError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mout\u001b[0m \u001b[0mof\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m   \"\"\"\n\u001b[1;32m--> 436\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mexpand_dims\u001b[1;34m(input, axis, name)\u001b[0m\n\u001b[0;32m   2282\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2283\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2284\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2285\u001b[0m         _ctx, \"ExpandDims\", name, input, axis)\n\u001b[0;32m   2286\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_reward_hist = []\n",
    "episode_loss_hist = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "\n",
    "    step = 0\n",
    "    state = env.reset() #list(env.decode(env.reset()))\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    # play and record in memory\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = select_action(episode, model, state, min_epsilon, max_epsilon, decay_rate)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        memory.add((state, action, new_state, reward, done, info))\n",
    "                \n",
    "        \n",
    "        if done:\n",
    "            state = env.reset() #list(env.decode(env.reset()))\n",
    "        else:\n",
    "            state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    episode_reward_hist.append(episode_reward)\n",
    "    \n",
    "    # sample from memory and train\n",
    "    q_target, states, actions = get_minibatch(model_target, memory, gamma, batch_size)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        q_values = model(states)\n",
    "        \n",
    "        masks = tf.one_hot(actions, env.action_space.n)\n",
    "        \n",
    "        q_action = tf.reduce_max(tf.multiply(q_values, masks), axis=1)\n",
    "\n",
    "        loss = tf.keras.losses.MeanSquaredError()(q_target, q_action)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    episode_loss_hist.append(loss)\n",
    "\n",
    "    if episode % 50 == 0:\n",
    "        model_target.set_weights(model.get_weights())\n",
    "\n",
    "    if episode % 100 ==0:\n",
    "        print(f\"Episode: {episode}\\tLoss: {loss:.3f}\\tEpisode reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(episode_reward_hist)\n",
    "plt.grid()\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(episode_loss_hist)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation , rc\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "frame = []\n",
    "env.reset()\n",
    "\n",
    "for episode in range(1):\n",
    "\n",
    "    step = 0\n",
    "    state = env.reset() #list(env.decode(env.reset()))\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "\n",
    "    # play and record in memory\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        action = select_action(episode, model, state, 0, 0, decay_rate)\n",
    "\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        img = plt.imshow(env.render('rgb_array'))\n",
    "        frame.append([img])\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset() \n",
    "        else:\n",
    "            state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "    print(f\"episode lasted {step} steps\")\n",
    "an = animation.ArtistAnimation(fig, frame, interval=100, repeat_delay=1000, blit=True)\n",
    "rc('animation', html='jshtml')\n",
    "an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
