{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformers.ipynb","provenance":[],"collapsed_sections":["aNWzHGgA0nyf"],"mount_file_id":"1dAycfA3ldhHktD80zKUNXalR0QuSCadC","authorship_tag":"ABX9TyNz/IuvTqj0znylODa7ms0m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"swDUGQDm7dfe"},"source":["# Transformers\n","\n","Paper: [Attention is all you need](https://arxiv.org/abs/1706.03762)\n","\n","Resources:\n","- [The Illustrated Transformer\n","](https://jalammar.github.io/illustrated-transformer/)\n","- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)\n","- [TF Docs: Transformer model for language understanding\n","](https://www.tensorflow.org/text/tutorials/transformer)\n","\n","#### Summary\n","- Network based only on attention, without recurrence or convolutions.\n","- Addresses long sequence dependence problem of recurrent models with single encoder hidden state.\n","- \n","\n","### Approach\n","\n","- Until done:\n","    1. Read the paper\n","    2. Try to implement until stuck\n","    3. Check resources\n","    4. Go to 1.\n"]},{"cell_type":"code","metadata":{"id":"CGY6wp3dAJOq","executionInfo":{"status":"ok","timestamp":1623375361733,"user_tz":360,"elapsed":2003,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import tensorflow_datasets as tfds\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"YqhzHM9wAHag","executionInfo":{"status":"ok","timestamp":1623375361735,"user_tz":360,"elapsed":23,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Zg0LYfqACrO"},"source":["# Attention model\n","\n","### Notation\n","- $Q$ = Query\n","- $K$ = Key, dimension $d_k$ = 64\n","- $V$ = Value, dimension $d_v$ = 64\n","- $n$ = input sequence length\n","- $m$ = output sequence length\n","- $d_\\text{model}$ = model dimension = 512\n","- $d_{ff}$ = inner layer dimension = 2048\n","- $h$ = heads\n","\n","\n","\n","## Attention\n","\n","### Basics\n","- Deals with problem of relationships between elements of a long sequence experienced by recurrent models.\n","- Pass all hidden states (all encoder output) to decoder.\n","- Decoder scores each of the encoder hidden states, applies a softmax to the score, then multiplies the score times the hidden state from the encoder.\n","- Rather than having a single hidden state from a recurrent sequence, the decoder has *all* the hidden states and a score (or attention) value for each of them to weight their importance.\n","- Self-attention means the network is learning to associate the relationships between input elements. \n","\n","### Multi-head attention\n","- Query, Key, Value (Q,K,V) for each of the encoder input vector created by the embedding.\n","- Attention value is the softmax of Query x Key scaled by the dimension $d_k$ then multiplied by Value\n","\n","$$\\text{Attention(Q,K,V)} = \\text{softmax}_k(\\frac{QK^T}{\\sqrt{d_k}})V$$\n","\n","- Multiple heads - number of attention layers running in parallel \n","\n","<img src=\"https://www.researchgate.net/publication/333078019/figure/fig1/AS:758304078839808@1557805189409/left-Scaled-Dot-Product-Attention-right-Multi-Head-Attention.png\" height=300 />\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aNWzHGgA0nyf"},"source":["## Attention"]},{"cell_type":"code","metadata":{"id":"YdGIapvC0VNx","executionInfo":{"status":"ok","timestamp":1623375361736,"user_tz":360,"elapsed":20,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# Attention\n","# sec 3.2.1\n","def scaled_dot_product_attention(q, k, v, mask):\n","    # get dimensions of the input, cast from tensor to float\n","    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n","    \n","    # compute queries x keys and scale by dimension\n","    scaled_attention = tf.matmul(q, k, transpose_b=True) / tf.sqrt(d_k)\n","    # print(f\"scaled attention shape {scaled_attention.shape}\")\n","\n","    # apply decoder mask\n","    if mask:\n","        scaled_attention += (mask * 1e-9)\n","\n","    # normalize all scores\n","    attention_weights = tf.nn.softmax(scaled_attention, axis=-1)\n","    # print(f\"attention shape {attention_weights.shape}\")\n","\n","    # times value\n","    output = tf.matmul(attention_weights, v)\n","    # print(f\"output shape {output.shape}\")\n","\n","    return output, attention_weights"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1fsCrjh02cu"},"source":["Use the `scaled_dot_product_attention` layer to get a handle on how the model is selecting query-key pairs and computing their value.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Puj-EKEBwG4Y","executionInfo":{"status":"ok","timestamp":1623375361985,"user_tz":360,"elapsed":266,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"a4bc6ccf-97dd-42d6-e1d9-430beb897541"},"source":["\n","def print_out(q, k, v):\n","    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n","    print('Attention weights are:')\n","    print(np.round(temp_attn, decimals=2))\n","    print('Output is:')\n","    print(np.round(temp_out, decimals=2))\n","\n","temp_k = tf.constant([[10, 0, 0],\n","                      [0, 10, 0],\n","                      [0, 0, 10],\n","                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[1, 0],\n","                      [10, 0],\n","                      [100, 5],\n","                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n","\n","\n","# The dot product attention is selecting the key that aligns with the \n","# query and then returning the associated value.\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","[[0. 1. 0. 0.]]\n","Output is:\n","[[10.  0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BS0EmTsw0qQi"},"source":["## Multi-head attention"]},{"cell_type":"code","metadata":{"id":"fcjJuOdxAHt7","executionInfo":{"status":"ok","timestamp":1623375361986,"user_tz":360,"elapsed":12,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["d_model = 512\n","d_ff = 2048\n","h = 8\n","d_k = 64\n","d_v = 64\n","\n","# sec 3.2.2\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0 \n","\n","        self.depth = self.d_model  // num_heads\n","\n","        self.wq = layers.Dense(d_model)\n","        self.wk = layers.Dense(d_model)\n","        self.wv = layers.Dense(d_model)\n","\n","        self.dense = layers.Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        \"\"\"\n","        The inputs need to be reshaped in order to be fed into the attention portion.\n","        The model dimension d_k get split into heads x depth.\n","        Then transposed to (batch_size, num_heads, seq_len, depth)\n","\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    # forward computation\n","    def call(self, q, k, v, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q) # (batch_size, seq_len, d_model)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","        print(q.shape)\n","\n","        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention,\n","                                    (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","\n","        return output, attention_weights\n","\n","mha = MultiHeadAttention(d_model, h)\n","\n","\n","\n","# Encoder\n","# "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZXWw8OQrT2T","executionInfo":{"status":"ok","timestamp":1623375362208,"user_tz":360,"elapsed":233,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"cb2d22f6-98c5-4e82-d6b5-5161684a062b"},"source":["x = tf.random.normal((1,60, 512), dtype=tf.float32)\n","\n","out, attn = mha(x,x,x, False)\n","out.shape, attn.shape\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(1, 60, 512)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"txzzj49L_30r"},"source":["# Data\n","- Used English-German and English-French dataset. 4.5 million and 36 million sentences respectively\n","- 37000 word English-German vocab\n","- 32000 word English-French vocab\n","- Instead let's use the TF dataset for Russian to English"]},{"cell_type":"code","metadata":{"id":"xcFqTEr83YFW","executionInfo":{"status":"ok","timestamp":1623375362209,"user_tz":360,"elapsed":18,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["# examples, metadata = tfds.load('ted_hrlr_translate/ru_to_en', with_info=True,\n","#                                as_supervised=True)\n","# train_examples, val_examples = examples['train'], examples['validation']"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"zkFVV1l956It","executionInfo":{"status":"ok","timestamp":1623375362210,"user_tz":360,"elapsed":17,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["# print an examples\n","# for ru, en in train_examples.take(1):\n","#   print(\"Russian: \", ru.numpy().decode('utf-8'))\n","#   print(\"English:   \", en.numpy().decode('utf-8'))"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7M6umRAu6e9t"},"source":["### Tokenization\n","- There's no tokenizer for the `ted hrlr` Russian to English set that I can find so I have to make one.\n","- The Google docs example for attention said they used a sub-word version built with Bert.\n","- I'll just use the example here https://www.tensorflow.org/text/guide/subwords_tokenizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TioATfFL8tdX","executionInfo":{"status":"ok","timestamp":1623375366720,"user_tz":360,"elapsed":4524,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"e1ea2db6-ce67-46b9-e0e9-c45026754480"},"source":["!pip install -q -U tensorflow-text"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 4.3MB 15.3MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WFL54Yfg614k","executionInfo":{"status":"ok","timestamp":1623375366976,"user_tz":360,"elapsed":266,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","# train_en = train_examples.map(lambda pt, en: en)\n","# train_ru = train_examples.map(lambda ru, en: ru)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"wFxzKMJi8xbD","executionInfo":{"status":"ok","timestamp":1623375438088,"user_tz":360,"elapsed":174,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","# bert_vocab_args = dict(\n","#     # The target vocabulary size\n","#     vocab_size = 8000,\n","#     # Reserved tokens that must be included in the vocabulary\n","#     reserved_tokens=reserved_tokens,\n","#     # Arguments for `text.BertTokenizer`\n","#     bert_tokenizer_params=bert_tokenizer_params,\n","#     # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","#     learn_params={},\n","# )"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plzF-nFw9Bps","executionInfo":{"status":"ok","timestamp":1623375438315,"user_tz":360,"elapsed":46,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"43859f04-aabd-446c-99a6-4b3959265138"},"source":["%%time\n","# en_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_en.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":21,"outputs":[{"output_type":"stream","text":["CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n","Wall time: 6.91 µs\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t2Hkjven9CSD","executionInfo":{"status":"ok","timestamp":1623375438316,"user_tz":360,"elapsed":38,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["# print(en_vocab[:10])\n","# print(en_vocab[100:110])\n","# print(en_vocab[1000:1010])\n","# print(en_vocab[-10:])"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"QDzCJsx-9CeJ","executionInfo":{"status":"ok","timestamp":1623375438316,"user_tz":360,"elapsed":36,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["# %%time\n","# ru_vocab = bert_vocab.bert_vocab_from_dataset(\n","#     train_ru.batch(1000).prefetch(2),\n","#     **bert_vocab_args\n","# )"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"NobcBT1P9CmQ","executionInfo":{"status":"ok","timestamp":1623375438316,"user_tz":360,"elapsed":33,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["# print(ru_vocab[:10])\n","# print(ru_vocab[100:110])\n","# print(ru_vocab[1000:1010])\n","# print(ru_vocab[-10:])"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSajpI4T9CuF","executionInfo":{"status":"ok","timestamp":1623375438317,"user_tz":360,"elapsed":32,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYVqjS_b9C1J","executionInfo":{"status":"ok","timestamp":1623375438317,"user_tz":360,"elapsed":32,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["# write_vocab_file('en_vocab.txt', en_vocab)\n","# write_vocab_file('ru_vocab.txt', ru_vocab)\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"aL8jJgld9C8L","executionInfo":{"status":"ok","timestamp":1623375438318,"user_tz":360,"elapsed":30,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":[""],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0pWFxlt9DDS","executionInfo":{"status":"ok","timestamp":1623375438318,"user_tz":360,"elapsed":28,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZDO_0OdXuMZy","executionInfo":{"status":"ok","timestamp":1623375694500,"user_tz":360,"elapsed":492,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"892d105f-9941-4fae-d3d4-e90b9d34ae0d"},"source":["!wget "],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2021-06-11 01:41:34--  https://drive.google.com/file/d/1CWpmbIjMvJPz5muaIs14lKM2i9rKdw0L/view?usp=sharing\n","Resolving drive.google.com (drive.google.com)... 172.253.123.101, 172.253.123.102, 172.253.123.138, ...\n","Connecting to drive.google.com (drive.google.com)|172.253.123.101|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘view?usp=sharing’\n","\n","view?usp=sharing        [ <=>                ]  65.11K  --.-KB/s    in 0.003s  \n","\n","2021-06-11 01:41:34 (20.3 MB/s) - ‘view?usp=sharing’ saved [66676]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GubNU_HK_dX7","colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"status":"error","timestamp":1623375438565,"user_tz":360,"elapsed":274,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}},"outputId":"99ef3f0f-4b8f-4067-df0a-55a629f80526"},"source":["\n","\n","tokenizers = tf.Module()\n","tokenizers.ru = CustomTokenizer(reserved_tokens, 'ru_vocab.txt')\n","tokenizers.en = CustomTokenizer(reserved_tokens, 'en_vocab.txt')"],"execution_count":28,"outputs":[{"output_type":"error","ename":"NotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-0dd76d2ffab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mru\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreserved_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ru_vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreserved_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'en_vocab.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-27-9c08a434738c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, reserved_tokens, vocab_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreserved_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBertTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reserved_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreserved_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAsset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/python/ops/bert_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_lookup_table, suffix_indicator, max_bytes_per_word, max_chars_per_token, token_out_type, unknown_token, split_unknown_characters, lower_case, keep_whitespace, normalization_form, preserve_unused_token, basic_tokenizer_class)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mvocab_lookup_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix_indicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_bytes_per_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmax_chars_per_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_out_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munknown_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         split_unknown_characters)\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_with_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_text/python/ops/wordpiece_tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_lookup_table, suffix_indicator, max_bytes_per_word, max_chars_per_token, token_out_type, unknown_token, split_unknown_characters)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlookup_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextFileIdTableInitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_lookup_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m       vocab_lookup_table = lookup_ops.StaticVocabularyTableV1(\n\u001b[0;32m--> 146\u001b[0;31m           init, num_oov_buckets=1, lookup_key_dtype=dtypes.string)\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_lookup_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLookupInterface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initializer, num_oov_buckets, lookup_key_dtype, name)\u001b[0m\n\u001b[1;32m   1265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackable_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrackable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_track_trackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initializer, default_value, name)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"hash_table\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStaticHashTable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, default_value, initializer)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initializer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/lookup_ops.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, table)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             self._delimiter, self._offset)\n\u001b[0m\u001b[1;32m    758\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         init_op = gen_lookup_ops.initialize_table_from_text_file_v2(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_lookup_ops.py\u001b[0m in \u001b[0;36minitialize_table_from_text_file_v2\u001b[0;34m(table_handle, filename, key_index, value_index, vocab_size, delimiter, offset, name)\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6895\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6896\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6897\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6898\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n","\u001b[0;31mNotFoundError\u001b[0m: ru_vocab.txt; No such file or directory [Op:InitializeTableFromTextFileV2]"]}]},{"cell_type":"code","metadata":{"id":"Q_bxYzmM_il8","executionInfo":{"status":"aborted","timestamp":1623375438558,"user_tz":360,"elapsed":9,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["model_name = 'ted_hrlr_translate_ru_en_converter'\n","tf.saved_model.save(tokenizers, model_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cmLu95UP_oLK","executionInfo":{"status":"aborted","timestamp":1623375438561,"user_tz":360,"elapsed":12,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["reloaded_tokenizers = tf.saved_model.load(model_name)\n","reloaded_tokenizers.en.get_vocab_size().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN7GpSbX_rhe","executionInfo":{"status":"aborted","timestamp":1623375438562,"user_tz":360,"elapsed":13,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":["tokens = reloaded_tokenizers.en.tokenize(['Hello TensorFlow!'])\n","tokens.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmfNEG-_1j0r"},"source":["## Embedding\n","- Learned embeddings convert tokens to vectors of dimension $d_\\text{model}$"]},{"cell_type":"markdown","metadata":{"id":"GQlcI-Y91k3P"},"source":["## Position encoding"]},{"cell_type":"markdown","metadata":{"id":"EwkWYTQ1A0jg"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"BRRLenCxtfMw"},"source":["## Full Encoder\n","- 6 layers\n","- 2 sub-layers per layer\n","    - Multi-head self attention\n","    - FC Feed-forward\n","    - Residual connections around each sub-layer\n","\n"]},{"cell_type":"code","metadata":{"id":"LJBow0-_1RiP","executionInfo":{"status":"aborted","timestamp":1623375438562,"user_tz":360,"elapsed":13,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-F8ZPl9w0g02"},"source":["## Decoder\n","- 6 layers\n","- Same 2 layers as Encoder but with a third layer between\n","    - Middle layer performs multi-head attention on output from Encoder\n","    - Same residual connections"]},{"cell_type":"markdown","metadata":{"id":"Pj-VpGSNADa9"},"source":["# Train\n","\n","## Optimizer\n","- Adam $\\beta_1$ = 0.9, $\\beta_2$ = 0.98 and $\\epsilon$ = $10^{-9}$"]},{"cell_type":"code","metadata":{"id":"4MJwCKHuAH_N","executionInfo":{"status":"aborted","timestamp":1623375438563,"user_tz":360,"elapsed":14,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-U9XOspyAF1s","executionInfo":{"status":"aborted","timestamp":1623375438565,"user_tz":360,"elapsed":16,"user":{"displayName":"Tim Sullivan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgDOmSXGsobJcA9UOtHRwU-sWjSZffSAqqvuM2IHQ=s64","userId":"11783081476684672772"}}},"source":[""],"execution_count":null,"outputs":[]}]}